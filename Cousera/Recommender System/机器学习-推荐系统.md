
# Recommender System

## Content Based Recommendations

### Problem formulation

- $r(i,j)=1$ if user $j$ has rated movie $i$ (0 otherwise)
- $y^{(i,j)}=$ rating by user $j$ on movie $i$ (if defined)
- $\theta^{(j)}=$ parameter vector for user $j$
- $x^{(i)}=$ feature vector for movie $i$
- For user $j$, movie $i$, predicted rating: $(\theta^{(j)})^T(x^{(i)})$
- $m^{(j)}=$ no. of movies rated by user $j$

### Optimization objective

- To learn $\theta^{(j)}:$ (parameter for user $j$):
$$\mathop{\min}_{\theta^{(j)}}\frac{1}{2}\sum_{i:r(i,j)=1}\left ( (\theta^{(j)})^Tx^{(i)} - y^{(i,j)} \right )^2 + \frac{\lambda}{2}\sum_{k=1}^n(\theta_k^{(j)})^2$$

- To learn $\theta^{(1)},\theta^{(2)},\dots,\theta^{(n_u)}$:
$$\mathop{\min}_{\theta^{(1)},\dots,\theta^{(n_u)}}\frac{1}{2}\sum_{j=1}^{n_u}\sum_{i:r(i,j)=1}\left ( (\theta^{(j)})^Tx^{(i)} - y^{(i,j)} \right )^2 + \frac{\lambda}{2}\sum_{j=1}^{n_u}\sum_{k=1}^n(\theta_k^{(j)})^2$$

### Optimization algorithm

**Gradient descent update:**

$$\begin{align}
& \theta_k^{(j)} := \theta_k^{(j)} - \alpha \sum_{i:r(i,j)=1}((\theta^{(j)})^Tx^{(i)}-y^{(i,j)})x_k^{(i)}(\mbox{ for k = 0})\\
& \theta_k^{(j)} := \theta_k^{(j)} - \alpha \left( \sum_{i:r(i,j)=1}((\theta^{(j)})^Tx^{(i)}-y^{(i,j)})x_k^{(i)} + \lambda \theta_k^{(j)} \right)(\mbox{ for }  k \ne 0)
\end{align}$$

## Collaborative filtering

### Optimization objective

- Given $x^{(1)},\dots,x^{(n_m)}$,can estimate $\theta^{(1)},\dots,\theta^{(n_u)}$
$$\mathop{\min}_{\theta^{(1)},\dots,\theta^{(n_u)}}\frac{1}{2}\sum_{j=1}^{n_u}\sum_{i:r(i,j)=1}\left ( (\theta^{(j)})^Tx^{(i)} - y^{(i,j)} \right )^2 + \frac{\lambda}{2}\sum_{j=1}^{n_u}\sum_{k=1}^n(\theta_k^{(j)})^2$$

- Given $\theta^{(1)},\dots,\theta^{(n_u)}$, estimate $x^{(1)},\dots,x^{(n_m)}$
$$\mathop{\min}_{x^{(1)},\dots,x^{(n_m)}}\frac{1}{2}\sum_{j=1}^{n_m}\sum_{j:r(i,j)=1}\left ( (\theta^{(j)})^Tx^{(i)} - y^{(i,j)} \right )^2 + \frac{\lambda}{2}\sum_{i=1}^{n_m}\sum_{k=1}^n(x_k^{(i)})^2$$

- Minimizing $x^{(1)},\dots,x^{(n_m)}$ and $\theta^{(1)},\dots,\theta^{(n_u)}$ simultaneously:

$$J(x^{(1)},\dots,x^{(n_m)},\theta^{(1)},\dots,\theta^{(n_u)})=\frac{1}{2}\sum_{(i,j):r(i,j)=1}\left ( (\theta^{(j)})^Tx^{(i)} - y^{(i,j)} \right )^2 + \frac{\lambda}{2}\sum_{i=1}^{n_m}\sum_{k=1}^n(x_k^{(i)})^2 + \frac{\lambda}{2}\sum_{j=1}^{n_u}\sum_{k=1}^n(\theta_k^{(j)})^2 \\
\mathop{\min}_{\begin{align}x^{(1)},\dots,x^{(n_m)}\\ \theta^{(1)},\dots,\theta^{(n_u)}\end{align}}J(x^{(1)},\dots,x^{(n_m)},\theta^{(1)},\dots,\theta^{(n_u)})$$

### Algorithm

1. Initialize $x^{(1)},\dots,x^{(n_m)},\theta^{(1)},\dots,\theta^{(n_u)}$ to small random values.
2. Minimize $J(x^{(1)},\dots,x^{(n_m)},\theta^{(1)},\dots,\theta^{(n_u)})$ using gradient descent (or an advanced optimization algorithm).E.g. for every $j=1,\dots,n_u,i=1,\dots,n_m$:
$$\begin{align}x_k^{(i)}=x_k^{(i)} - \alpha \left ( \sum_{j:r(i,j)=1}((\theta^{(j)})^Tx^{(i)}-y^{(i,j)})\theta_k^{(j)} + \lambda x_k^{(i)} \right ) \\
\theta_k^{(j)} := \theta_k^{(j)} - \alpha \left( \sum_{i:r(i,j)=1}((\theta^{(j)})^Tx^{(i)}-y^{(i,j)})x_k^{(i)} + \lambda \theta_k^{(j)} \right)
\end{align}$$
3. For a user with parameters $\theta$ and a movie with (learned) features $x$, predict a star rating of $\theta^Tx$.

## Vectorization

| Movie | Alice(1) | Bob(2) | Carol(3) | Dave(4) |
|--------|--------|
|Love at last|5|5|0|0|
|Romance forever|5|?|?|0|
|Cute pupples of love|?|4|0|?|
|Nonstop car chases|0|0|5|4|
|Swords vs. karate|0|0|5|?|

$$Y = \begin{bmatrix}
5 & 5 & 0 & 0 \\
5 & ? & ? & 0 \\
? & 4 & 0 & ? \\
0 & 0 & 5 & 4 \\
0 & 0 & 5 & 0
\end{bmatrix}$$

**Predicted ratings:**

$$\begin{bmatrix}
(\theta^{(1)})^T(x^{(1)}) & (\theta^{(2)})^T(x^{(1)}) & \cdots &  (\theta^{(n_u)})^T(x^{(1)}) \\
(\theta^{(1)})^T(x^{(2)}) & (\theta^{(2)})^T(x^{(2)}) & \cdots &  (\theta^{(n_u)})^T(x^{(2)}) \\
\vdots & \vdots & \vdots & \vdots \\
(\theta^{(n_m)})^T(x^{(n_m)}) & (\theta^{(2)})^T(x^{(n_m)}) & \cdots &  (\theta^{(n_u)})^T(x^{(n_m)})
\end{bmatrix}$$

### Low rank matrix factorization

$$X = \begin{bmatrix}
-(x^{(1)})^T-\\
-(x^{(2)})^T-\\
\vdots\\
-(x^{(n_m)})^T-\\
\end{bmatrix} \quad \Theta=\begin{bmatrix}
-(\Theta^{(1)})^T-\\
-(\Theta^{(2)})^T-\\
\vdots\\
-(\Theta^{(n_u)})^T-
\end{bmatrix}$$

### Mean Normalization

$$Y = \begin{bmatrix}
5 & 5 & 0 & 0 & ? \\
5 & ? & ? & 0 & ?  \\
? & 4 & 0 & ? & ?  \\
0 & 0 & 5 & 4 & ?  \\
0 & 0 & 5 & 0 & ? 
\end{bmatrix} \quad \mu = \begin{bmatrix}
2.5\\
2.5\\
2\\
2.25\\
1.25
\end{bmatrix} \to Y-\mu$$

we use $Y - \mu$ to learn $\Theta^{(j)},x^{(i)}$

For user $j$, on movie $i$ predict:
$$(\Theta^{(j)})^T(x^{(i)})+\mu_i$$


