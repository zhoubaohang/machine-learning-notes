

## Debugging a learning algorithm

- 假设你已经实现了正则化的线性回归模型来预测房价。
- 然而，当你在新的房价数据集上测试假设函数时，发现了不可接受的大误差。那接下来你将怎么办？

**Approaches**

1. 获取更多训练数据集
2. 尝试使用更少的特征
3. 尝试添加一些特征
4. 尝试添加多项式特征
5. 尝试增大 $\lambda$
6. 尝试减小 $\lambda$

## Evaluating your hypothesis

- 我们将数据集分割出训练集与测试集，一般这个比例设为 $7 : 3$。

**E.g.**

$$\begin{align}
(x^{(1)}&,y^{(1)})\\
(x^{(2)}&,y^{(2)})\\
\vdots\\
\vdots\\
(x^{(m)}&,y^{(m)})
\end{align}$$

_ _ _

$$\begin{align}
(x_{test}^{(1)}&,y_{test}^{(1)})\\
(x_{test}^{(2)}&,y_{test}^{(2)})\\
\vdots\\
(x_{test}^{(m_{test})}&,y_{test}^{(m_{test})})
\end{align}$$

### Training/testing procedure for linear regression

- Learn parameter $\theta$ from training data (minimizing training error $J(\theta)$)
- Compute test set error:

$$J_{test}(\theta)=\frac{1}{2m_{test}} \sum_{i=1}^{m_{test}}(h_\theta (x_{test}^{(i)})-y_{test}^{(i)})^2$$

### Training/testing procedure for logistic regression

- Learn parameter $\theta$ from training data
- Compute test set error:

$$J_{test}(\theta)= - \frac{1}{m_{test}} \sum_{i=1}^{m_{test}}y_{test}^{(i)} \log h_\theta(x_{test}^{(i)})+(1-y_{test}^{(i)}) \log h_\theta(1-x_{test}^{(i)})$$

- Missclassification error (0/1 misclassification error):

$$err(h_\theta(x),y)=
\begin{cases}
1 & \mbox{if } h_\theta(x) \ge 0.5,  \mbox{y=0} \\
& \mbox{or if } h_\theta(x) < 0.5,  \mbox{y=1} \\
0 & \mbox{otherwise}
\end{cases} \\
Test_{error} = \frac{1}{m} \sum_{i=1}^{m_{test}}err(h_\theta(x_{test}^{(i)}),y^{(i)})
$$

## Model selection and training/validation/test sets

- 更为一般的方法是 将 数据集 分割为 训练集、验证集、测试集，比例为：$60 \%, 20 \%, 20 \%$

**E.g.**

$$\begin{align}
(x^{(1)}&,y^{(1)})\\
(x^{(2)}&,y^{(2)})\\
\vdots\\
\vdots\\
(x^{(m)}&,y^{(m)})
\end{align}$$

_ _ _

$$\begin{align}
(x_{cv}^{(1)}&,y_{cv}^{(1)})\\
(x_{cv}^{(2)}&,y_{cv}^{(2)})\\
\vdots\\
(x_{cv}^{(m_{cv})}&,y_{cv}^{(m_{cv})})
\end{align}$$
_ _ _

$$\begin{align}
(x_{test}^{(1)}&,y_{test}^{(1)})\\
(x_{test}^{(2)}&,y_{test}^{(2)})\\
\vdots\\
(x_{test}^{(m_{test})}&,y_{test}^{(m_{test})})
\end{align}$$

### Train/validation/test error

**Training error:**

$$J_{train}(\theta)= \frac{1}{m} \sum_{i=1}^m(h_\theta(x^{(i)})-y^{(i)})^2$$

**Cross validation error:**

$$J_{cv}(\theta)=\frac{1}{2m_{cv}} \sum_{i=1}^{m_{cv}}(h_\theta(x_{cv}^{(i)})-y_{cv}^{(i)})^2$$

**Test error:**

$$J_{test}(\theta)=\frac{1}{2m_{test}}\sum_{i=1}^{m_{test}}(h_\theta(x_{test}^{(i)})-y_{test}^{(i)})^2$$


### Model selection


$$\begin{align}
\mbox{1. } \quad & h_\theta(x)=\theta_0+\theta_1x\\
\mbox{2. } \quad & h_\theta(x)=\theta_0+\theta_1x+\theta_2x^2\\
\mbox{3. } \quad  & h_\theta(x)=\theta_0+\theta_1x+\theta_2x^2+\theta_3x^3\\
& \vdots\\
\mbox{10. } \quad & h_\theta(x)=\theta_0+\theta_1x+\cdots+\theta_{10}x^{10}
\end{align}$$

- 计算上面每种假设函数的验证误差，找出其中最小的一个，即为：将要使用的假设函数。

## Diagnosing Bias vs. Variance

- Suppose your learning algorithm is performing less well than you were hoping. ($J_{cv}(\theta)$ or $J_{test}(\theta)$ is high.) Is it a bias problem or a variance problem?
- 在吴老师的视频中，他用 error-degree of polynomial 图来讲解了偏差与方差问题。总的来说，偏差问题对应欠拟合，而方差问题对应过拟合。而这两种问题在 训练误差与验证误差上的表现很不同。因此，借助这种不同，我们可以来检查我们的模型出现了何种问题。

**Bias(underfit):**

$$\begin{align}
& J_{train}(\theta) \  \mbox{will be high}\\
& J_{cv}(\theta) \simeq J_{train}(\theta)
\end{align}$$

**Variance(overfit):**

$$\begin{align}
& J_{train}(\theta) \  \mbox{will be low}\\
& J_{cv}(\theta) \gg J_{train}(\theta)
\end{align}$$

## Regulation and Bias_Variance

**Linear regression with regularization**

Model:

$$\begin{align}
& h_\theta(x)=\theta_0+\theta_1x+\theta_2x^2+\theta_3x^3+\theta_4x^4\\
& J(\theta)=\frac{1}{2m}\sum_{i=1}^m (h_\theta(x^{(i)})-y^{(i)})^2+\frac{\lambda}{2m}\sum_{j=1}^m \theta_j^2
\end{align}$$

**Choosing the regularization parameter** $\lambda$

$$\begin{align}
& \mbox{1. Try } \lambda = 0\\
& \mbox{2. Try } \lambda = 0.01\\
& \mbox{3. Try } \lambda = 0.02\\
& \mbox{4. Try } \lambda = 0.04\\
& \mbox{5. Try } \lambda = 0.08\\
& \vdots\\
& \mbox{12. Try } \lambda = 10\\
\end{align}$$

1. 上面得到了12个模型，每个训练后得到的模型再去进行验证，得到了验证误差。
2. 选取误差最小的模型作为结果。

**Bias/variance as a function of the regularization parameter** $\lambda$

1. 随着 $\lambda$ 的增大，$J_{train}(\theta)$会逐渐增大，而$J_{cv}(\theta)$则会先下降后上升。
2. $\lambda$过小对应方差问题，而过大对应偏差问题。

## Learning curves

- 我们经常使用学习曲线来检查算法模型是否处于偏差、方差问题等。

**High bias**

1. 随着 m(training set size) 的增大，$J_{train}(\theta)$与$J_{cv}(\theta)$越来越接近，并且处于较高值。
2. 此时，即使使用更多的训练集也是没有意义的。

**High variance**

1. 随着 m(training set size) 的增大，$J_{train}(\theta)$与$J_{cv}(\theta)$间有较大距离，并会变得接近。
2. 此时使用更多的训练集也许会有帮助。


## Revisit

| 方法 | 目的 |
|--------|--------|
|获取更多训练数据|fix high variance|
|尝试使用更少的特征|fix high variance|
|尝试添加一些特征|fix high bias|
|尝试添加多项式特征|fix high bias|
|尝试增大 $\lambda$|fix high bias|
|尝试减小 $\lambda$|fix high variance|







